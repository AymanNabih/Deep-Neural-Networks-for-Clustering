{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Saman Paidar Nia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get important libraries for this class.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "#--------------------------------------------------------\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from IPython.display import clear_output\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from sklearn.preprocessing import normalize\n",
    "from numpy import linalg as LA\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "#---------------------------------------------------------\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Similarity_func:\n",
    "    def sigmoidal_normalize(self, V):\n",
    "        return (V - min(V)) / (max(V) - min(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cosine_Similarity(Similarity_func):\n",
    "    def get_matrix(self, X):\n",
    "        X = normalize(X, axis=0)\n",
    "        x = squareform(pdist(X, 'cosine'))\n",
    "        x = np.dot(LA.matrix_power(np.diag(np.sum(x, 0)), -1), x)\n",
    "        x = np.apply_along_axis(self.sigmoidal_normalize, 1, x)\n",
    "        np.fill_diagonal(x, 0.)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Similarity_Dataset_Iterator():\n",
    "    def __init__(self, data, labels, similarity):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.matrix = similarity.get_matrix(data)\n",
    "        self.data_size = self.matrix.shape[0]\n",
    "        self.current_index = 0\n",
    "    def next_batch(self, num):\n",
    "        data=self.matrix.transpose()\n",
    "        labels=self.labels\n",
    "        idx = np.arange(0 , len(data))\n",
    "        np.random.shuffle(idx)\n",
    "        idx = idx[:num]\n",
    "        data_shuffle = [data[ i] for i in idx]\n",
    "        labels_shuffle = [labels[ i] for i in idx]\n",
    "        return data_shuffle, labels_shuffle\n",
    "    def whole_dataset(self):\n",
    "        return (self.matrix.transpose(), self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Using Scikit-Learn libraries to fetching the Newsgroups data set: http://scikit-learn.org\n",
    "def read_NewsGroup_data(similarity):\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format='%(asctime)s %(levelname)s %(message)s')\n",
    "    op = OptionParser()\n",
    "    op.add_option(\"--lsa\", dest=\"n_components\", type=\"int\",\n",
    "                  help=\"Preprocess documents with latent semantic analysis.\")    \n",
    "    op.add_option(\"--no-idf\",action=\"store_false\", dest=\"use_idf\", default=True,\n",
    "                  help=\"Disable Inverse Document Frequency feature weighting.\")\n",
    "    op.add_option(\"--use-hashing\", action=\"store_true\", default=False,\n",
    "                  help=\"Use a hashing feature vectorizer\")\n",
    "    op.add_option(\"--n-features\", type=int, default=10000,\n",
    "                  help=\"Maximum number of features to extract from text.\")    \n",
    "    def is_interactive():\n",
    "        return not hasattr(sys.modules['__main__'], '__file__')\n",
    "    argv = [] if is_interactive() else sys.argv[1:]\n",
    "    (opts, args) = op.parse_args(argv)\n",
    "    if len(args) > 0:\n",
    "        op.error(\"this script takes no arguments.\")\n",
    "        sys.exit(1)\n",
    "    # Load some categories from the training set\n",
    "    categories_9NG = ['talk.politics.mideast', 'talk.politics.misc', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware',\n",
    "                      'sci.electronics', 'sci.crypt', 'sci.med', 'sci.space', 'misc.forsale']\n",
    "    # categories = categories_9NG\n",
    "    dataset = fetch_20newsgroups(subset='train', categories=categories_9NG,\n",
    "                                 shuffle=True, random_state=42)\n",
    "    labels = dataset.target[:1800]\n",
    "    true_k = np.unique(labels).shape[0]\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,min_df=2,\n",
    "                                 stop_words='english',use_idf=opts.use_idf)\n",
    "    X = vectorizer.fit_transform(dataset.data[:1800])\n",
    "    if opts.n_components:\n",
    "        svd = TruncatedSVD(opts.n_components)\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        lsa = make_pipeline(svd, normalizer)\n",
    "        X = lsa.fit_transform(X)\n",
    "        explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    return Similarity_Dataset_Iterator(X.toarray(), labels, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Cosine_Similarity as similarity dataset.\n",
    "data_set_train = read_NewsGroup_data(Cosine_Similarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_(X, n_cluster):\n",
    "    kmeans_centroids,_ =  kmeans(X,n_cluster)\n",
    "    kmeans_, _ = vq(X, kmeans_centroids)\n",
    "    return kmeans_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = data_set_train.data_size #--------- Number of input data.\n",
    "# Define the number of hidden layer. \n",
    "if n_input >= 1024:\n",
    "    Nn = int(2048)\n",
    "elif n_input >= 512:\n",
    "    Nn = int(1024)\n",
    "elif n_input >= 256:\n",
    "    Nn = int(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = int(Nn/2) #-------------------- The autoencoder hidden layer 1.\n",
    "n_hidden_2 = int(n_hidden_1/2) #------------ The autoencoder hidden layer 2.\n",
    "n_hidden_3 = int(n_hidden_2/2) #------------ The autoencoder hidden layer 3.\n",
    "n_hidden_4 = int(n_hidden_3/2) #------------ The autoencoder hidden layer 4.\n",
    "n_code = str(int(n_hidden_4/2)) #----------- The number of output dimension value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, n_code, phase_train):    \n",
    "    with tf.variable_scope(\"encoder\"):        \n",
    "        with tf.variable_scope(\"hidden_1\"):\n",
    "            hidden_1 = layer(x, [n_input, n_hidden_1], [n_hidden_1], phase_train)\n",
    "        with tf.variable_scope(\"hidden_2\"):\n",
    "            hidden_2 = layer(hidden_1, [n_hidden_1, n_hidden_2], [n_hidden_2], phase_train)\n",
    "        with tf.variable_scope(\"hidden_3\"):\n",
    "            hidden_3 = layer(hidden_2, [n_hidden_2, n_hidden_3], [n_hidden_3], phase_train)\n",
    "        with tf.variable_scope(\"hidden_4\"):\n",
    "            hidden_4 = layer(hidden_3, [n_hidden_3, n_hidden_4], [n_hidden_4], phase_train)\n",
    "        with tf.variable_scope(\"code\"):\n",
    "            code = layer(hidden_4, [n_hidden_4, n_code], [n_code], phase_train)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(code, n_code, phase_train):\n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        with tf.variable_scope(\"hidden_1\"):\n",
    "            hidden_1 = layer(code, [n_code, n_hidden_4], [n_hidden_4], phase_train)\n",
    "        with tf.variable_scope(\"hidden_2\"):\n",
    "            hidden_2 = layer(hidden_1, [n_hidden_4, n_hidden_3], [n_hidden_3], phase_train)\n",
    "        with tf.variable_scope(\"hidden_3\"):\n",
    "            hidden_3 = layer(hidden_2, [n_hidden_3, n_hidden_2], [n_hidden_2], phase_train)\n",
    "        with tf.variable_scope(\"hidden_4\"):\n",
    "            hidden_4 = layer(hidden_3, [n_hidden_2, n_hidden_1], [n_hidden_1], phase_train)\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            output = layer(hidden_4, [n_hidden_1, n_input], [n_input], phase_train)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_batch_norm(x, n_out, phase_train):\n",
    "    beta_init = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
    "    gamma_init = tf.constant_initializer(value=1.0, dtype=tf.float32)\n",
    "    beta = tf.get_variable(\"beta\", [n_out], initializer=beta_init)\n",
    "    gamma = tf.get_variable(\"gamma\", [n_out], initializer=gamma_init)\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "    ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "    def mean_var_with_update():\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "    mean, var = control_flow_ops.cond(phase_train, mean_var_with_update, lambda: (ema_mean, ema_var))\n",
    "    reshaped_x = tf.reshape(x, [-1, 1, 1, n_out])\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(reshaped_x, mean, var, beta, gamma, 1e-3, True)\n",
    "    return tf.reshape(normed, [-1, n_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(input, weight_shape, bias_shape, phase_train):\n",
    "    weight_init = tf.random_normal_initializer(stddev=(1.0 / weight_shape[0]) ** 0.5)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    logits = tf.matmul(input, W) + b\n",
    "    return tf.nn.sigmoid(layer_batch_norm(logits, weight_shape[1], phase_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output, x):\n",
    "    with tf.variable_scope(\"training\"):\n",
    "        train_loss = tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(output, x)), 1))\n",
    "        return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output, x):\n",
    "    with tf.variable_scope(\"validation\"):\n",
    "        val_loss = tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(output, x, name=\"val_diff\")), 1))\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost, learning_rate, beta1, beta2, global_step):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon=1e-08, use_locking=False, name='Adam')\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_layers = 6 #-------------------------- Number of Neural Networks Layers.\n",
    "beta1 = 0.9 #------------------------------- The decay rate 1.  \n",
    "beta2 = 0.999 #----------------------------- The decay rate 2.\n",
    "learning_rate = (beta1/n_input) #----------- The learning rate.\n",
    "n_batch = math.ceil(n_input/(n_layers**2)) #-- Number of selection data in per step.\n",
    "n_backpro = math.ceil(n_input/n_batch) #---- Number of Backpro in per epoc\n",
    "n_epoch = math.ceil(n_input/(n_layers*2)) #----- The time priod of train.\n",
    "n_cluster = 9 #----------------------------- Number of clusters.\n",
    "n_diplay = round(n_epoch/(n_layers*2)) #-------- Number of runnig the K-Means and NMI.\n",
    "results=[] #-------------------------------- A list to keep all NMI scores.\n",
    "loss_cost=[] #------------------------------ A list to keep all training evaluations.\n",
    "steps=[] #---------------------------------- A list to keep all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():    \n",
    "    with tf.variable_scope(\"autoencoder_model\"):                \n",
    "        x = tf.placeholder(\"float\", [None, n_input])   \n",
    "        phase_train = tf.placeholder(tf.bool)\n",
    "        code = encoder(x, int(n_code), phase_train)\n",
    "        output = decoder(code, int(n_code), phase_train)\n",
    "        cost = loss(output, x)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_optimizer = training(cost, learning_rate, beta1, beta2, global_step)\n",
    "        eval_optimizer = evaluate(output, x)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        sess = tf.Session()\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▉                                                                           | 11/150 [01:55<24:16, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI Score for AE is: 0.31 and new cost is: 354.04 in 12 step. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▍                                                                    | 23/150 [04:08<22:50, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI Score for AE is: 0.30 and new cost is: 290.76 in 24 step. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████▉                                                              | 35/150 [05:57<19:35, 10.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI Score for AE is: 0.21 and new cost is: 234.23 in 36 step. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████████████████▍                                                       | 47/150 [07:33<16:33,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI Score for AE is: 0.23 and new cost is: 229.82 in 48 step. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████████████████████▊                                                 | 59/150 [09:09<14:08,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI Score for AE is: 0.22 and new cost is: 169.07 in 60 step. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████████████████████▋                                             | 66/150 [10:09<12:55,  9.23s/it]"
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "for epoch in tqdm(range(1, n_epoch+1)):\n",
    "    # Fit training with Backpropagation using batch data.\n",
    "    for i in range(n_backpro):\n",
    "        miniData, _ = data_set_train.next_batch(n_batch)\n",
    "        _,new_cost = sess.run([train_optimizer, cost], feed_dict={x: miniData,\n",
    "                                                                  phase_train: True})\n",
    "       #---------------- End of the Backpropagation with Random Walk ----------------\n",
    "    randomData, _ = data_set_train.next_batch(n_input)\n",
    "    sess.run(eval_optimizer, feed_dict={x: randomData,\n",
    "                                        phase_train: True})\n",
    "    #------------------------- End of the Optimization ---------------------------\n",
    "    # Save the results after per (Size of input / n_layers) epochs.    \n",
    "    if epoch % n_diplay == 0 or epoch == n_epoch:\n",
    "        # Getting embedded codes and running K-Means on them.\n",
    "        ae_codes = sess.run(code, feed_dict={x: data_set_train.whole_dataset()[0],\n",
    "                                         phase_train: False})        \n",
    "        idx = k_means_(ae_codes, n_cluster)\n",
    "        ae_nmi = normalized_mutual_info_score(data_set_train.whole_dataset()[1], idx)\n",
    "        results.append(ae_nmi)    \n",
    "        steps.append(epoch)\n",
    "        loss_cost.append(new_cost)    \n",
    "        print(\"NMI Score for AE is: {:0.2f} and new cost is: {:0.2f} in {:d} step. \"\n",
    "              .format(ae_nmi, new_cost, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.errorbar(steps, loss_cost, label='Autoencoder Cost Trianing', color='r')\n",
    "plt.ylabel('NMI')\n",
    "plt.grid()\n",
    "plt.title(('Cost Function Trianing after {:d} epochs is {:0.2f}').format(n_epoch,new_cost))\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(1,2,2)\n",
    "plt.errorbar(steps, results, label='Autoencoder on Normalized Cosine Similarity', color='g')\n",
    "plt.ylabel('NMI')\n",
    "plt.grid()\n",
    "plt.title(('NMI of AE after {:d} epochs is {:0.2f}').format(n_epoch,ae_nmi))\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
