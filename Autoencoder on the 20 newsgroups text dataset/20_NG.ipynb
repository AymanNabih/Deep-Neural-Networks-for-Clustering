{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Saman Paidar Nia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All resources are listed at the bottom of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get important libraries for this class.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import math\n",
    "import logging\n",
    "import sys\n",
    "#--------------------------------------------------------\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from IPython.display import clear_output\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from sklearn.preprocessing import normalize\n",
    "from numpy import linalg as LA\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from math import sqrt\n",
    "#---------------------------------------------------------\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from optparse import OptionParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Similarity_Normalization:\n",
    "    def normalization(self, X):\n",
    "        return (X - min(X)) / (max(X) - min(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Correlation_Similarity(Similarity_Normalization):\n",
    "    def get_matrix(self, X):\n",
    "        X = normalize(X, axis=0)\n",
    "        X = pdist(X, 'correlation')\n",
    "        X = squareform(X)\n",
    "        S = np.sum(X, 0)\n",
    "        D = np.diag(S)\n",
    "        D = LA.matrix_power(D, -1)\n",
    "        Y = np.dot(D, X)\n",
    "        Y = np.apply_along_axis(self.normalization, 1, Y)\n",
    "        np.fill_diagonal(Y, 0.)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cosine_Similarity(Similarity_Normalization):\n",
    "    def get_matrix(self, X):\n",
    "        X = normalize(X, axis=0)\n",
    "        X = pdist(X, 'cosine')\n",
    "        X = squareform(X)\n",
    "        S = np.sum(X, 0)\n",
    "        D = np.diag(S)\n",
    "        D = LA.matrix_power(D, -1)\n",
    "        Y = np.dot(D, X)\n",
    "        Y = np.apply_along_axis(self.normalization, 1, Y)\n",
    "        np.fill_diagonal(Y, 0.)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Similarity_Dataset_Iterator():\n",
    "    def __init__(self, data, labels, similarity):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.matrix = similarity.get_matrix(data)\n",
    "        self.data_size = self.matrix.shape[0]\n",
    "        self.current_index = 0\n",
    "    def next_batch(self, num):\n",
    "        data=self.matrix.transpose()\n",
    "        labels=self.labels\n",
    "        idx = np.arange(0 , len(data))\n",
    "        np.random.shuffle(idx)\n",
    "        idx = idx[:num]\n",
    "        data_shuffle = [data[ i] for i in idx]\n",
    "        labels_shuffle = [labels[ i] for i in idx]\n",
    "        return data_shuffle, labels_shuffle\n",
    "    def whole_dataset(self):\n",
    "        return (self.matrix.transpose(), self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Using Scikit-Learn libraries to fetching the Newsgroups data set: http://scikit-learn.org\n",
    "def read_NewsGroup_data(similarity):    \n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "    op = OptionParser()\n",
    "    op.add_option(\"--lsa\", dest=\"n_components\", type=\"int\",\n",
    "                  help=\"Preprocess documents with latent semantic analysis.\")    \n",
    "    op.add_option(\"--no-idf\",action=\"store_false\", dest=\"use_idf\", default=True,\n",
    "                  help=\"Disable Inverse Document Frequency feature weighting.\")\n",
    "    op.add_option(\"--use-hashing\", action=\"store_true\", default=False,\n",
    "                  help=\"Use a hashing feature vectorizer\")\n",
    "    op.add_option(\"--n-features\", type=int, default=10000,\n",
    "                  help=\"Maximum number of features to extract from text.\")    \n",
    "    def is_interactive():\n",
    "        return not hasattr(sys.modules['__main__'], '__file__')\n",
    "    argv = [] if is_interactive() else sys.argv[1:]\n",
    "    (opts, args) = op.parse_args(argv)\n",
    "    if len(args) > 0:\n",
    "        op.error(\"this script takes no arguments.\")\n",
    "        sys.exit(1)\n",
    "    dataset = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n",
    "    labels = dataset.target[:4000]\n",
    "    true_k = np.unique(labels).shape[0]\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,min_df=2,\n",
    "                                 stop_words='english',use_idf=opts.use_idf)\n",
    "    X = vectorizer.fit_transform(dataset.data[:4000])\n",
    "    if opts.n_components:\n",
    "        svd = TruncatedSVD(opts.n_components)\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        lsa = make_pipeline(svd, normalizer)\n",
    "        X = lsa.fit_transform(X)\n",
    "        explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    return Similarity_Dataset_Iterator(X.toarray(), labels, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Cosine_Similarity as similarity dataset.\n",
    "trainSet_cosine = read_NewsGroup_data(Cosine_Similarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Correlation_Similarity as similarity dataset.\n",
    "trainSet_correlation = read_NewsGroup_data(Correlation_Similarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = trainSet_cosine.data_size #--------- Number of input data.\n",
    "# Define the number of hidden layer. \n",
    "if n_input >= 2048:\n",
    "    Nn = int(4096)\n",
    "elif n_input >= 1024:\n",
    "    Nn = int(2048)\n",
    "elif n_input >= 512:\n",
    "    Nn = int(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = int(Nn/2) #-------------------- The autoencoder hidden layer 1.\n",
    "n_hidden_2 = int(n_hidden_1/2) #------------ The autoencoder hidden layer 2.\n",
    "n_hidden_3 = int(n_hidden_2/2) #------------ The autoencoder hidden layer 3.\n",
    "n_code = str(int(n_hidden_3/2)) #----------- The number of output dimension value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:  4000\n",
      "Layer 2:  2048\n",
      "Layer 3:  1024\n",
      "Layer 4:  512\n",
      "Layer 5:  256\n"
     ]
    }
   ],
   "source": [
    "print('Layer 1: ', n_input)\n",
    "print('Layer 2: ', n_hidden_1)\n",
    "print('Layer 3: ', n_hidden_2)\n",
    "print('Layer 4: ', n_hidden_3)\n",
    "print('Layer 5: ', int(n_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_(X, n_cluster):\n",
    "    kmeans_centroids,_ =  kmeans(X,n_cluster)\n",
    "    kmeans_, _ = vq(X, kmeans_centroids)\n",
    "    return kmeans_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_clustering(X, n_cluster):\n",
    "    D_ = np.matrix(np.diag(np.sum(X, 0)))\n",
    "    D_ = D_**(-1)\n",
    "    L = np.dot(np.dot(D_, X), D_)\n",
    "    eigenvectors = np.linalg.eig(L)[1]\n",
    "    X = np.real(eigenvectors[:, 0: n_cluster])\n",
    "    rows_norm = np.linalg.norm(X, axis=1, ord=0)\n",
    "    X = (X.T / rows_norm).T\n",
    "    return k_means_(X, n_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, n_code, phase_train):    \n",
    "    with tf.variable_scope(\"encoder\"):        \n",
    "        with tf.variable_scope(\"hidden_1\"):\n",
    "            hidden_1 = layer(x, [n_input, n_hidden_1], [n_hidden_1], phase_train)\n",
    "        with tf.variable_scope(\"hidden_2\"):\n",
    "            hidden_2 = layer(hidden_1, [n_hidden_1, n_hidden_2], [n_hidden_2], phase_train)\n",
    "        with tf.variable_scope(\"hidden_3\"):\n",
    "            hidden_3 = layer(hidden_2, [n_hidden_2, n_hidden_3], [n_hidden_3], phase_train)        \n",
    "        with tf.variable_scope(\"code\"):\n",
    "            code = layer(hidden_3, [n_hidden_3, n_code], [n_code], phase_train)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(code, n_code, phase_train):\n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        with tf.variable_scope(\"hidden_1\"):\n",
    "            hidden_1 = layer(code, [n_code, n_hidden_3], [n_hidden_3], phase_train)\n",
    "        with tf.variable_scope(\"hidden_2\"):\n",
    "            hidden_2 = layer(hidden_1, [n_hidden_3, n_hidden_2], [n_hidden_2], phase_train)\n",
    "        with tf.variable_scope(\"hidden_3\"):\n",
    "            hidden_3 = layer(hidden_2, [n_hidden_2, n_hidden_1], [n_hidden_1], phase_train)              \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            output = layer(hidden_3, [n_hidden_1, n_input], [n_input], phase_train)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_batch_norm(x, n_out, phase_train):\n",
    "    beta_init = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
    "    gamma_init = tf.constant_initializer(value=1.0, dtype=tf.float32)\n",
    "    beta = tf.get_variable(\"beta\", [n_out], initializer=beta_init)\n",
    "    gamma = tf.get_variable(\"gamma\", [n_out], initializer=gamma_init)\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "    ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "    def mean_var_with_update():\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "    mean, var = control_flow_ops.cond(phase_train, mean_var_with_update, lambda: (ema_mean, ema_var))\n",
    "    reshaped_x = tf.reshape(x, [-1, 1, 1, n_out])\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(reshaped_x, mean, var, beta, gamma, 1e-3, True)\n",
    "    return tf.reshape(normed, [-1, n_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(input, weight_shape, bias_shape, phase_train):\n",
    "    weight_init = tf.random_normal_initializer(stddev=(1.0 / weight_shape[0]) ** 0.5)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    logits = tf.matmul(input, W) + b\n",
    "    return tf.nn.sigmoid(layer_batch_norm(logits, weight_shape[1], phase_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output, x):\n",
    "    with tf.variable_scope(\"training\"):\n",
    "        train_loss = tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(output, x)), 1))\n",
    "        return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost, learning_rate, beta1, beta2, global_step):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon=1e-08, use_locking=False, name='Adam')\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_layers = 5 #----------------------------- Number of Neural Networks Layers.\n",
    "beta1 = 0.9 #------------------------------ The decay rate 1.  \n",
    "beta2 = 0.999 #---------------------------- The decay rate 2.\n",
    "learning_rate = (beta1/n_input) #---------- The learning rate.\n",
    "stop_learning = 1.35 #--------------------- The stop learning point.\n",
    "n_batch = math.ceil(sqrt(sqrt(n_input))) #- Number of selection data in per step.\n",
    "n_backpro = math.ceil(n_input/n_batch) #--- Number of Backpro in per epoch.\n",
    "n_cluster = 20 #--------------------------- Number of clusters.\n",
    "n_diplay = 10 #---------------------------- Number of getting code and runnig the K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cor=[] #--------------------------- A list to keep all NMI scores.\n",
    "loss_cost_cor=[] #------------------------- A list to keep all training evaluations.\n",
    "steps_cor=[] #----------------------------- A list to keep all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():    \n",
    "    with tf.variable_scope(\"autoencoder_model\"):                \n",
    "        x = tf.placeholder(\"float\", [None, n_input])   \n",
    "        phase_train = tf.placeholder(tf.bool)\n",
    "        code = encoder(x, int(n_code), phase_train)\n",
    "        output = decoder(code, int(n_code), phase_train)\n",
    "        cost = loss(output, x)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_optimizer = training(cost, learning_rate, beta1, beta2, global_step)\n",
    "        sess = tf.Session()\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI Score for AE is: 0.33 and new cost is: 257.88 in 10 step. \n",
      "NMI Score for AE is: 0.35 and new cost is: 62.17 in 20 step. \n",
      "NMI Score for AE is: 0.38 and new cost is: 14.14 in 30 step. \n",
      "NMI Score for AE is: 0.49 and new cost is: 3.80 in 40 step. \n",
      "NMI Score for AE is: 0.59 and new cost is: 1.90 in 50 step. \n",
      "NMI Score for AE is: 0.59 and new cost is: 1.54 in 60 step. \n",
      "NMI Score for AE is: 0.60 and new cost is: 1.44 in 70 step. \n",
      "NMI Score for AE is: 0.61 and new cost is: 1.33 in 75 step. \n"
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "epoch = 0\n",
    "first_pass = True\n",
    "while first_pass or new_cost >= stop_learning:\n",
    "    first_pass = False\n",
    "    # Fit training with Backpropagation using batch data.\n",
    "    for i in range(n_backpro):\n",
    "        miniData, _ = trainSet_correlation.next_batch(n_batch)\n",
    "        _, new_cost = sess.run([train_optimizer,cost], feed_dict={x: miniData,\n",
    "                                                                  phase_train: True})       \n",
    "    #------------------------- End of the Optimization ------------------------------\n",
    "    epoch += 1\n",
    "    # Save the results after per 10 epochs.    \n",
    "    if epoch % n_diplay == 0 or new_cost <= stop_learning:\n",
    "        # Getting embedded codes and running K-Means on them.\n",
    "        ae_codes = sess.run(code, feed_dict={x: trainSet_correlation.whole_dataset()[0],\n",
    "                                             phase_train: False})        \n",
    "        idx = k_means_(ae_codes, n_cluster)\n",
    "        ae_nmi_cor = normalized_mutual_info_score(trainSet_correlation.whole_dataset()[1], idx)\n",
    "        results_cor.append(ae_nmi_cor)    \n",
    "        steps_cor.append(epoch)\n",
    "        loss_cost_cor.append(new_cost)    \n",
    "        print(\"NMI Score for AE is: {:0.2f} and new cost is: {:0.2f} in {:d} step. \"\n",
    "              .format(ae_nmi_cor, new_cost, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cos=[] #--------------------------- A list to keep all NMI scores.\n",
    "loss_cost_cos=[] #------------------------- A list to keep all training evaluations.\n",
    "steps_cos=[] #----------------------------- A list to keep all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():    \n",
    "    with tf.variable_scope(\"autoencoder_model\"):                \n",
    "        x = tf.placeholder(\"float\", [None, n_input])   \n",
    "        phase_train = tf.placeholder(tf.bool)\n",
    "        code = encoder(x, int(n_code), phase_train)\n",
    "        output = decoder(code, int(n_code), phase_train)\n",
    "        cost = loss(output, x)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_optimizer = training(cost, learning_rate, beta1, beta2, global_step)\n",
    "        sess = tf.Session()\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI Score for AE is: 0.32 and new cost is: 268.83 in 10 step. \n",
      "NMI Score for AE is: 0.33 and new cost is: 66.90 in 20 step. \n",
      "NMI Score for AE is: 0.33 and new cost is: 16.52 in 30 step. \n",
      "NMI Score for AE is: 0.48 and new cost is: 4.46 in 40 step. \n",
      "NMI Score for AE is: 0.54 and new cost is: 2.00 in 50 step. \n",
      "NMI Score for AE is: 0.53 and new cost is: 1.53 in 60 step. \n",
      "NMI Score for AE is: 0.55 and new cost is: 1.46 in 70 step. \n",
      "NMI Score for AE is: 0.55 and new cost is: 1.59 in 80 step. \n",
      "NMI Score for AE is: 0.57 and new cost is: 1.43 in 90 step. \n",
      "NMI Score for AE is: 0.58 and new cost is: 1.66 in 100 step. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-8e941860532c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mminiData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainSet_cosine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         _, new_cost = sess.run([train_optimizer,cost], feed_dict={x: miniData,\n\u001b[1;32m---> 10\u001b[1;33m                                                                   phase_train: True})       \n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m#------------------------- End of the Optimization ------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\users\\saman\\anaconda3\\envs\\tensorflow3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\users\\saman\\anaconda3\\envs\\tensorflow3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\users\\saman\\anaconda3\\envs\\tensorflow3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\users\\saman\\anaconda3\\envs\\tensorflow3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\users\\saman\\anaconda3\\envs\\tensorflow3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "epoch = 0\n",
    "first_pass = True\n",
    "while first_pass or new_cost >= stop_learning:\n",
    "    first_pass = False\n",
    "    # Fit training with Backpropagation using batch data.\n",
    "    for i in range(n_backpro):\n",
    "        miniData, _ = trainSet_cosine.next_batch(n_batch)\n",
    "        _, new_cost = sess.run([train_optimizer,cost], feed_dict={x: miniData,\n",
    "                                                                  phase_train: True})       \n",
    "    #------------------------- End of the Optimization ------------------------------\n",
    "    epoch += 1\n",
    "    # Save the results after per 10 epochs.    \n",
    "    if epoch % n_diplay == 0 or new_cost <= stop_learning:\n",
    "        # Getting embedded codes and running K-Means on them.\n",
    "        ae_codes = sess.run(code, feed_dict={x: trainSet_cosine.whole_dataset()[0],\n",
    "                                             phase_train: False})        \n",
    "        idx = k_means_(ae_codes, n_cluster)\n",
    "        ae_nmi_cos = normalized_mutual_info_score(trainSet_cosine.whole_dataset()[1], idx)\n",
    "        results_cos.append(ae_nmi_cos)    \n",
    "        steps_cos.append(epoch)\n",
    "        loss_cost_cos.append(new_cost)    \n",
    "        print(\"NMI Score for AE is: {:0.2f} and new cost is: {:0.2f} in {:d} step. \"\n",
    "              .format(ae_nmi_cos, new_cost, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spec_cluster_cos = spectral_clustering(trainSet_cosine.whole_dataset()[0], n_cluster)\n",
    "spec_nmi_cos = normalized_mutual_info_score(trainSet_cosine.whole_dataset()[1], spec_cluster_cos)\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "k_means_indx_cos = k_means_(trainSet_cosine.whole_dataset()[0], n_cluster)\n",
    "k_means_nmi_cos = (normalized_mutual_info_score(trainSet_cosine.whole_dataset()[1], k_means_indx_cos))\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "spec_cluster_cor = spectral_clustering(trainSet_correlation.whole_dataset()[0], n_cluster)\n",
    "spec_nmi_cor = normalized_mutual_info_score(trainSet_correlation.whole_dataset()[1], spec_cluster_cor)\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "k_means_indx_cor = k_means_(trainSet_correlation.whole_dataset()[0], n_cluster)\n",
    "k_means_nmi_cor = (normalized_mutual_info_score(trainSet_correlation.whole_dataset()[1], k_means_indx_cor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.errorbar(steps_cor, loss_cost_cor, label='Cost Trianing for Correlation Distance ', color='#000080', marker='o')\n",
    "plt.errorbar(steps_cos, loss_cost_cos, label='Cost Trianing for Cosine Distance ', color='#E3CF57', marker='o')\n",
    "plt.xlabel('Number of Epochs.')\n",
    "plt.ylabel('Cost')\n",
    "plt.grid()\n",
    "plt.title('Cost Function Trianing')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(1,2,2)\n",
    "plt.errorbar(steps_cor, results_cor, label='AE Normalized Correlation Distance ', color='#000080', marker='o')\n",
    "plt.errorbar(steps_cos, results_cos, label='AE Normalized Cosine Distance ', color='#E3CF57', marker='o')\n",
    "plt.xlabel('Number of Epochs.')\n",
    "plt.ylabel('NMI')\n",
    "plt.grid()\n",
    "plt.title(('NMI of AE Correlation is {:0.2f} and AE Cosine is {:0.2f}').format(ae_nmi_cor*100, ae_nmi_cos*100))\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KMeans on Cosine: ---------------------------- {:0.2f}\".format(k_means_nmi_cos*100))\n",
    "print(\"KMeans on Correlation: ----------------------- {:0.2f}\".format(k_means_nmi_cor*100))\n",
    "print(\"Spectral Clustering on Cosine: --------------- {:0.2f}\".format(spec_nmi_cos*100))\n",
    "print(\"Spectral Clustering on Correlation: ---------- {:0.2f}\".format(spec_nmi_cor*100))\n",
    "print(\"Autoencoder Clustering on Cosine: ------------ {:0.2f}\".format(ae_nmi_cos*100))\n",
    "print(\"Autoencoder Clustering on Correlation: ------- {:0.2f}\".format(ae_nmi_cor*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refrences:\n",
    "\n",
    "SciPy (the library) Jones E, Oliphant E, Peterson P, et al. SciPy: Open Source Scientific Tools for Python, 2001-, http://www.scipy.org/ [Online; accessed 2018-02-20]. Here’s an example of a BibTeX entry:\n",
    "\n",
    "@Misc{, author = {Eric Jones and Travis Oliphant and Pearu Peterson and others}, title = {{SciPy}: Open source scientific tools for {Python}}, year = {2001--}, url = \"http://www.scipy.org/\", note = {[Online; accessed\n",
    "\n",
    "NumPy & SciPy:\n",
    "\n",
    "Stéfan van der Walt, S. Chris Colbert and Gaël Varoquaux. The NumPy Array: A Structure for Efficient Numerical Computation, Computing in Science & Engineering, 13, 22-30 (2011), DOI:10.1109/MCSE.2011.37 (publisher link)\n",
    "\n",
    "IPython:\n",
    "\n",
    "Fernando Pérez and Brian E. Granger. IPython: A System for Interactive Scientific Computing, Computing in Science & Engineering, 9, 21-29 (2007), DOI:10.1109/MCSE.2007.53 (publisher link)\n",
    "\n",
    "Matplotlib:\n",
    "\n",
    "John D. Hunter. Matplotlib: A 2D Graphics Environment, Computing in Science & Engineering, 9, 90-95 (2007), DOI:10.1109/MCSE.2007.55 (publisher link)\n",
    "\n",
    "Scikit-learn:\n",
    "\n",
    "Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, Édouard Duchesnay. Scikit-learn: Machine Learning in Python, Journal of Machine Learning Research, 12, 2825-2830 (2011) (publisher link)\n",
    "\n",
    "TensorFlow:\n",
    "\n",
    "@misc{tensorflow2015-whitepaper, title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems}, url={https://www.tensorflow.org/}, note={Software available from tensorflow.org},\n",
    "\n",
    "Jupyter Notebooks:\n",
    "\n",
    "@conference{Kluyver:2016aa, Author = {Thomas Kluyver and Benjamin Ragan-Kelley and Fernando P{\\'e}rez and Brian Granger and Matthias Bussonnier and Jonathan Frederic and Kyle Kelley and Jessica Hamrick and Jason Grout and Sylvain Corlay and Paul Ivanov and Dami{\\'a}n Avila and Safia Abdalla and Carol Willing}, Booktitle = {Positioning and Power in Academic Publishing: Players, Agents and Agendas}, Editor = {F. Loizides and B. Schmidt}, Organization = {IOS Press}, Pages = {87 - 90}, Title = {Jupyter Notebooks -- a publishing format for reproducible computational workflows}, Year = {2016}}"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
